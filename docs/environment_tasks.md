# Environment Tasks & Rollout Functions

Gradients now supports **Environment Tasks**, leveraging the custom rollout capabilities within the TRL `GRPOTrainer`. This framework enables models to interact with external environments during the training loop to optimize based on dynamic feedback and sparse rewards.

## Evaluation Protocol

Post-training, Gradients assesses model performance by executing **250 episodes** within the target environment. The primary metric for success is the **average score** across this full evaluation set.


## Miner Requirements

During training, miners are provisioned with environment servers hosting specific task logic. To maximize throughput and minimize latency, one dedicated environment server is typically allocated per GPU.

### 1. Environment Connectivity

Server addresses are exposed via the `ENVIRONMENT_SERVER_URLS` environment variable as a comma-separated string.

**Python Implementation:**

```python
import os

# Extract and clean server URLs
raw_urls = os.environ.get("ENVIRONMENT_SERVER_URLS", "")
server_list = [url.strip() for url in raw_urls.split(",") if url.strip()]

```

### 2. The Rollout Function Workflow

Miners must implement a custom **Rollout Function** associated with the `dataset_type.environment_name`. This function manages the following lifecycle:

* **Sampling:** Generate model completions via `generate_rollout_completions`.
* **Execution:** Interface with environment servers using these completions to drive state changes.
* **Synchronization:** Return a dictionary containing prompt tokens, completion tokens, logprobs, and reward signals to the trainer.

> [!TIP]
> **Optimizing for GRPO Grouping:** Understanding how GRPO groups trajectories is critical for policy stability. Top-performing miners leverage grouping to maximize training efficiency, especially in complex, multi-turn environments.

> [!IMPORTANT]
> **Supported Environments:** The currently supported environment for the tournament is the [Affine GAME Environment](https://github.com/AffineFoundation/affinetes/tree/main/environments/openspiel). Specifically, the Goofspiel Game (Task IDs 0-99999999).


### 3. Configuration

Declare your Rollout Function within your **Axolotl config** or to your trainer directly if you are not using Axolotl, mirroring the syntax used for standard GRPO Reward Functions. A reference implementation is available in `dockerfiles/environment_functions`. You must also pass a reward function, usually the actual reward calculation is done within your rollout function and then passed to the reward function as kwargs.

**Example Reward Function:**

```python
def rollout_reward_func(completions, **kwargs):
    rewards = kwargs.get("env_rewards") if kwargs else None
    return [float(r) for r in rewards] if rewards is not None else [0.0] * len(completions)
```

Note: this example expects the rollout function to include in its dictionary a list of the rewards for each episode under the key `env_rewards`.

> [!IMPORTANT]
> **Custom Miner Images:** If you are not using the base miner implementation we provide ensure that you have the correct versions of TRL and VLLM installed that support the experimental `generate_rollout_completions` feature.


## Testing & Validation

To ensure local parity with Gradients' evaluation systems, two utility scripts are provided:

* **Local Training:** Use `examples/run_environment_task.sh` to trigger a training run in a local environment.
* **Standardized Evaluation:** Use `scripts/manual_environment_eval.py` to evaluate your trained model using the exact parameters and episode count used by Gradients.


## Advice

The default miner implementation leaves plenty of room for optimizations and is very inefficient. Here are some ideas for ways to improve the miner and win your first tournament.

* **Train on entire episode:** Currently the default miner only trains on the tokens generated by the first prompt and the first completion from the model. Expanding this to train on the tokens from the entire episode will greatly improve training.
* **Inter-Episode Rewards:** Currently the only rewards the model receives are from the environment server, which for the most part gives 0 reward until a completion state is reached. Finding a way to give the model intermediate reward will greatly improve training.
* **Better Multi-GPU:** To use rollout functions TRL requires the use of VLLM for model inference during training. Improving how the training script uses VLLM (colocate, running VLLM server on only a single gpu and the rest for training, etc) can greatly improve efficiency.


## Rules

Code submissions to the environment tournaments must adhere to the following rules. Submissions will be checked for compliance. Any winner of the tournament that breaks one of the following rules may have their win revoked. These rules may be changed in the future.

1. You may not bring your own dataset in the docker image.

2. You may not bring a pretrained model in the docker image.

3. You may not do any SFT for environment tasks.

We have these rules to ensure that the winner is competing in the intended spirit of the tournament: to create training scripts that use live interactions with an environment to train a model.

As always we are open to a continual conversation with our community about these rules and how to ensure the tournaments are fair. We will make a public announcement any time we have to enact these rules and strip a winner of their title.


## Technical References

| Resource | Description |
| --- | --- |
| **[Affinetes](https://github.com/AffineFoundation/affinetes)** | The standard protocol for Gradients environment server communication. |
| **[Affine GAME Environment](https://github.com/AffineFoundation/affinetes/tree/main/environments/openspiel)** | The currently used environment server miners are given access to during training. |
| **[OpenEnv Rollout Functions](https://huggingface.co/docs/trl/main/en/openenv)** | TRL's official guide on implementing custom rollout logic. |

---